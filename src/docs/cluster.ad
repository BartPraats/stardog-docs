= High Availability Cluster

In this section we explain how to configure, use, and administer Stardog Cluster
for uninterrupted operations.

== Stardog Cluster

Stardog Cluster is a collection of Stardog Server instances running on one or
more virtual or physical machines that, *from the client's perspective*, behave
like a single Stardog Server instance.footnote:[To fully achieve this effect
requires,  DNS or HaProxy or similar environmental configuration that's left as
an exercise for the user.] Of course Stardog Cluster should have some different
operational properties, the main one of which is high availability. But from the
client's perspective Stardog Cluster should be indistinguishable from
non-clustered Stardog.footnote:[The client here means the client of Stardog
APIs, not necessarily people, administrators, etc.]

NOTE: High Availability requires *at least* three nodes in the Cluster. Stardog
Cluster works best with an odd-number greater than or equal to three cluster
size with respect to fault resiliency: 3, 5, 7, etc.footnote:["Because Zookeeper
requires a majority, it is best to use an odd number of machines. For example,
with four machines ZooKeeper can only handle the failure of a single machine; if
two machines fail, the remaining two machines do not constitute a majority.
However, with five machines ZooKeeper can handle the failure of two machines."
See https://zookeeper.apache.org/doc/r3.1.2/zookeeperAdmin.html[Zk Admin] for
more.] With respect to performance, larger cluster sizes
perform better than smaller ones.

== Configuration

To deploy Stardog Cluster you use `stardog-admin` commands and some additional
configuration. Stardog Cluster depends on Apache ZooKeeper. In the following
installation notes, we install Stardog Cluster in a 3-node configuration on a
LAN. If you need larger cluster, adjust accordingly.footnote:[In order to deploy
a cluster to a single machine--for example, for testing or development--you must
ensure that each Stardog and ZooKeeper servers has a different home location.]

. <<Quick Start Guide,Install>> Stardog {version} on each machine in the cluster.
+
NOTE: The smart thing to do here, of course, is to use whatever infrastructure
you have in place to automate software installation. Adapting Stardog
installation to Chef, Puppet, cfengine, etc. is left as an exercise for the
reader.
+
. Make sure a valid Stardog license key (whether Developer, Enterprise, or a
30-day eval key) for the size of cluster you're creating exists and is each
`STARDOG_HOME`. You must also have a `stardog.properties` file with the following
information **for each node in the cluster**:
+
[source,shell]
----
# Flag to enable the cluster, without this flag set, the rest of the properties have no effect
pack.enabled=true
# the connection string for ZooKeeper where cluster state is stored
pack.cluster.address=196.69.68.1:2180,196.69.68.2:2180,196.69.68.3:2180
# credentials used for securing ZooKeeper state
pack.cluster.username=pack
pack.cluster.password=admin
----
+
`pack.cluster.address` is a ZooKeeper connection string where cluster stores its
state. `pack.cluster.username` and `pack.cluster.password` are user and password
tokens for cluster communications and may be different from actual Stardog users
and passwords; however, **all nodes must use the same user and password
combination.**
+
. Create the ZooKeeper configuration for each node. This config file is just a standard ZooKeeper configuration file. The following
config file should be sufficient for most cases.
+
On node 1:
+
[source,shell]
----
tickTime=2000
# Make sure this directory exists and
# ZK can write and read to and from it.
dataDir=/tmp/zookeeperdata/
clientPort=2180
initLimit=5
syncLimit=2
# This is an enumeration of all nodes in
# the cluster and must be identical in
# each node's config.
server.1=196.69.68.1:2888:3888
server.2=196.69.68.2:2888:3888
server.3=196.69.68.3:2888:3888
----
+
On node 2:
+
[source,shell]
----
tickTime=2000
dataDir=/tmp/zookeeperdata/
clientPort=2181
initLimit=5
syncLimit=2
server.1=196.69.68.1:2180
server.2=196.69.68.3:2180
----
+
Finally, on node 3:
+
[source,shell]
----
tickTime=2000
dataDir=/tmp/zookeeperdata/
clientPort=2180
initLimit=5
syncLimit=2
server.1=196.69.68.1:2888:3888
server.2=196.69.68.2:2888:3888
server.3=196.69.68.3:2888:3888
----
+
NOTE: The `clientPort` specified in `zookeeper.properties` and the ports used in `pack.cluster.address` in
`stardog.properties` must be the same.
+
. `dataDir` is where ZooKeeper persists cluster state and where it
writes log information about the cluster.
+
[source,shell]
----
$ mkdir /tmp/zookeeperdata # on node 1
$ mkdir /tmp/zookeeperdata # on node 2
$ mkdir /tmp/zookeeperdata # on node 3
----
+
. ZooKeeper requires a `myid` file in the `dataDir` folder to identify itself, you will create that file as follows
for `node1` and `node2`, respectively:
+
[source,shell]
----
$ echo 1 > /tmp/zookeeperdata/myid # on node 1
$ echo 2 > /tmp/zookeeperdata/myid # on node 2
$ echo 3 > /tmp/zookeeperdata/myid # on node 3
----

== Installation

In the next few steps you will use the Stardog Admin CLI commands to deploy
Stardog Cluster: that is, ZooKeeper, the Proxy, and Stardog itself.

. To start ZooKeeper's part of Cluster, use the `stadog-admin cluster` subcommand:
+
[source,shell]
----
$ ./stardog-admin cluster zkstart --home ~/stardog # on node 1
$ ./stardog-admin cluster zkstart --home ~/stardog # on node 2
$ ./stardog-admin cluster zkstart --home ~/stardog # on node 3
----
+
Which uses the `zookeeper.properties` config file in `~/stardog` and log its
output to `~/stardog/zookeeper.log`. For more info about the command:
+
[source,shell]
----
$ ./stardog-admin help cluster zkstart
----
+
Once ZooKeeper is started, you can start Stardog Cluster:
+
[source,shell]
----
$ ./stardog-admin server start --home ˜/stardog --port 5820 # on node 1
$ ./stardog-admin server start --home ˜/stardog --port 5820 # on node 2
$ ./stardog-admin server start --home ˜/stardog --port 5820 # on node 3
----
+
. Start the Stardog Cluster Proxy:
+
[source,shell]
----
$ ./stardog-admin cluster proxystart --zkconnstr 196.69.68.1:2180,196.69.68.2:2180,196.69.68.3:2180 \
	--user pack --password admin --port 5820 # on node 1
$ ./stardog-admin cluster proxystart --zkconnstr 196.69.68.1:2180,196.69.68.2:2180,196.69.68.3:2180 \
   --user pack --password admin --port 5820 # on node 2
$ ./stardog-admin cluster proxystart --zkconnstr 196.69.68.1:2180,196.69.68.2:2180,196.69.68.3:2180 \
	--user pack --password admin --port 5820 # on node 3
----
+
Note that the `zkconnstr` option is the same connection string as
`pack.cluster.address` in `stardog.properties`, and `user` and `password` are
the same as `pack.cluster.username` and `pack.cluster.password`, respectively.
For more information on the proxy configuration execute:
+
[source,shell]
----
$ ./stardog-admin help cluster proxystart
----

Now Stardog Cluster is running on 3 nodes, one each on 3 machines. Since
the proxy was conveniently configured to use port `5820` you can execute
standard Stardog CLI commands to the Cluster:

[source,shell]
----
$ ./stardog-admin db create -n myDb
$ ./stardog data add myDb /path/to/my/data
$ ./stardog query myDb "select * { ?s ?p ?o } limit 5"
----

== Cluster Topologies & Cluster Size

In the configuration instructions above, we assume a particular Cluster
typology, which is to say, for each node `n` of a cluster, we run Stardog,
ZooKeeper, and a Proxy. But this is not the only typology supported by Stardog
Cluster.

ZooKeeper nodes run independently, so other typologies--three ZooKeeper servers
and five Stardog servers are possible--you just have to point Stardog to the
corresponding ZooKeeper cluster.

To add more Stardog Cluster nodes, simply repeat the steps for Stardog on
additional machines. Generally, as mentioned above, Stardog Cluster size should
be an odd number greater or equal to 3.

//<<Cluster Performance>>

== Stardog Cluster Client

To use Stardog Cluster with standard Stardog clients and CLI tools in the
ordinary way--`stardog-admin` and `stardog`--you must have Stardog installed
locally. With the provided Stardog binaries in the Stardog Cluster distribution
you can query the state of Cluster:footnote:[In 3.x we will integrate Netflix's
Exhibitor for Stardog Cluster management. Ping us if this is a priority for
you.]

[source,shell]
----
$ ./stardog-admin --server snarl://<ipaddress>:5820/ cluster info
----

where `ipaddress` is the IP address of any of the nodes in the cluster. This
will print the available nodes in the cluster, as well as the roles (participant
or coordinator). You can also input the proxy IP address and port to get the
same information.

To add or remove data, issue `stardog data add` or `remove` commands to
any node in the cluster. Queries can be issued to any node in the cluster using
the `stardog query` command. All the `stardog-admin` features are also available
in Cluster, which means you can use any of the commands to create
databases, adminster users, and the rest of the functionality.

== Troubleshooting ZooKeeper

ZooKeeper uses
http://zookeeper.apache.org/doc/r3.3.5/zookeeperInternals.html#sc_quorum[majority
quorums] for leader election by default which means that, at any given time, ZooKeeper
requires that there are at least *n/2 + 1* nodes when leader election is
happening. In the event that a Stardog Cluster is stuck in this
phase, you can use Starman to recover a ZooKeeper node using:

[source,shell]
----
$ ./starman cluster addnodes ==provider <provider> ==id <cluster id> ==zk-node
----

Starman will attempt to guess which of the ZooKeeper server(s) was lost and try
to bring a ZooKeeper server back--one at a time--with the same hostname.

Starman creates a list of hostnames when nodes are deployed to a cluster and
uses that list to communicate between nodes in the ZooKeeper cluster. Whenever a
ZooKeeper server is lost, a new one can be brought back without having to use
the same IP address, which may not be possible or expensive for some cloud
providers.

== Stardog Cluster Guarantees

Stardog Cluster implements an atomic commitment protocol based on
http://en.wikipedia.org/wiki/Two-phase_commit_protocol[two-phase commit (2PC)]
over a shared replicated memory that's provided by Apache ZooKeeper. A cluster is
composed of a set of Stardog servers running together. One of the servers is
known as the Coordinator and the rest as Participants.

In case the Coordinator fails at any point, a new Coordinator will be elected
out of the remaining available Participants. Stardog Cluster supports both
`read` (e.g., querying) and `write` (e.g., adding data) requests. Read requests
are load-balanced over the available Participants, whereas write requests are
transparently forwarded to and handled by the Coordinator. In some future
release we may change the protocol implemented by the Cluster and thus change
some of the allowable topologies, including multiple-writers and
multiple-readers.

When a client commits a transaction (containing a list of `write` requests), it
will be acknowledged by the Coordinator only **after every non-failing Participant
has committed the transaction**. If a Participant fails during the process of
committing a transaction, it will be expelled from the cluster by the
Coordinator and put in a temporary `failed` state.

If the Coordinator fails during the process, the transaction will be aborted,
and a new Coordinator will be elected automatically. Since `failed` nodes are
not used for any subsequent `read` or `write` requests, **if a commit is
acknowledged by the Coordinator, then Stardog Cluster guarantees that the data
has been accordingly modified at *every* available node in the cluster**.

While this approach is less performant with respect to write
operations than eventual consistency used by other distributed databases,
typically those databases offer a much less expressive data model than Stardog,
which makes an eventually consistency model more appropriate for those systems.
But since Stardog's data model is not only richly expressive but rests in part
on provably correct semantics, we think that a strong consistency model is worth
the cost.footnote:[Based on customer feedback we may relax these consistency
guarantees in some future release. Please get in touch if you think an
eventually consistent approach is more appropriate for yr use of Stardog.]
